{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import intercoder_reliability_functions as fun\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the annotation JSON file into a DataFrame\n",
    "df = pd.read_json(\"input/recoded_transcripts/main-at-2024-05-28-recoded-mostly.json\")\n",
    "\n",
    "#get intercoder reliability IDs list\n",
    "all_inner_IDs=pd.read_csv(\"input/recoded_transcripts/main_project_IDs.csv\")\n",
    "\n",
    "# Load JSON transcript data\n",
    "with open('input/news_segments_2020-04_2021-04_unique_IDs.json') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# Assuming each item in the list has a 'data' key, we extract those\n",
    "extracted_data = [item['data'] for item in data_list]\n",
    "\n",
    "# Normalize the data to flatten the nested structure\n",
    "transcript_df = pd.json_normalize(extracted_data)\n",
    "\n",
    "# unpack the meta_info\n",
    "expanded_columns = df['meta_info'].apply(pd.Series)\n",
    "df = pd.concat([df.drop('meta_info', axis=1), expanded_columns], axis=1)\n",
    "#filter the df based on index of annotations\n",
    "df = df[df['internal_id'].isin(all_inner_IDs[\"internal_id\"])]\n",
    "\n",
    "# define annotators\n",
    "annotator_list=df['annotator'].unique()\n",
    "annotator_dict = {\n",
    "    2: 'JE',\n",
    "    3: 'EK',\n",
    "    10: 'VR',\n",
    "    4: 'CA',\n",
    "    11: 'LU',\n",
    "    13: 'PI',\n",
    "    12: 'GE',\n",
    "    14: 'TR',\n",
    "    15: 'KA'\n",
    "}\n",
    "\n",
    "## create dataframe\n",
    "df= pd.merge(df, all_inner_IDs[['internal_id', 'project_id']], on='internal_id', how='left')\n",
    "df['media_outlet'] = df['source'].str.split().str[0]\n",
    "question_labels=['climate_change','attitude','policy']\n",
    "# Convert the 'meta_info.date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%B %d, %Y %A')\n",
    "\n",
    "\n",
    "df.to_csv(\"output/all_Q.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## define separate dataframes for each question\n",
    "Q1_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[0]).fillna(value=np.nan)\n",
    "Q2_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[1])\n",
    "Q3_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[2])\n",
    "\n",
    "# Save DataFrames as CSV files in the output directory\n",
    "Q1_df.to_csv(\"output/Q1_df.csv\")\n",
    "Q2_df.to_csv(\"output/Q2_df.csv\")\n",
    "Q3_df.to_csv(\"output/Q3_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## define choices for each question\n",
    "Q1_choices=[\"Acknowledges\",\"Neutral\",\"Denies\",\"Debate\",\"Unclear\"]\n",
    "Q2_choices=[\"Expresses climate concern\",\"Neutral\",\"Expresses opposition to climate concern\",\"Debate\",\"Unclear\"]\n",
    "Q3_choices=[\"Supports\",\"Neutral\",\"Opposes\",\"Debate\",\"Unclear\",\"Does not mention\"]\n",
    "\n",
    "\n",
    "# define short choices for displaying\n",
    "short_Q2_choices=['Concern',\n",
    " 'Neutral',\n",
    " 'Opposition',\n",
    " 'Debate',\n",
    " 'Unclear'] \n",
    "short_Q3_choices=[\"Supports\",\"Neutral\",\"Opposes\",\"Debate\",\"Unclear\",\"No mention\"]\n",
    "\n",
    "\n",
    "# Save lists to the output directory\n",
    "with open(\"output/Q1_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q1_choices, f)\n",
    "\n",
    "with open(\"output/Q2_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q2_choices, f)\n",
    "\n",
    "with open(\"output/Q3_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q3_choices, f)\n",
    "\n",
    "with open(\"output/short_Q2_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(short_Q2_choices, f)\n",
    "\n",
    "with open(\"output/short_Q3_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(short_Q3_choices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count responses \n",
    "def count_responses(Q_df):\n",
    "   # Flatten the DataFrame to consider all the responses\n",
    "    responses = Q_df.values.flatten()\n",
    "    # Filter out NaN values\n",
    "    responses = responses[~pd.isnull(responses)]\n",
    "\n",
    "    # Count occurrences of each unique response\n",
    "    response_counts = pd.Series(responses).value_counts()\n",
    "\n",
    "    # Assign the value 0.5 to each response because there are two annotators per response\n",
    "    response_values = response_counts * 0.5\n",
    "\n",
    "    # Convert to DataFrame for better presentation\n",
    "    response_values_df = response_values.reset_index()\n",
    "    \n",
    "    response_values_df.columns = ['Response', 'Value']\n",
    "    # Calculating the total value\n",
    "    total_value = response_values_df['Value'].sum()\n",
    "\n",
    "    # Adding the Proportion column\n",
    "    response_values_df['Proportion'] = response_values_df['Value'] / total_value\n",
    "\n",
    "    return response_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_count_Q1=count_responses(Q1_df)\n",
    "response_count_Q2=count_responses(Q2_df)\n",
    "response_count_Q3=count_responses(Q3_df)\n",
    "\n",
    "response_count_Q1.to_csv(\"output/response_count_Q1.csv\",index=False)\n",
    "response_count_Q2.to_csv(\"output/response_count_Q2.csv\",index=False)\n",
    "response_count_Q3.to_csv(\"output/response_count_Q3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify unique media outlets\n",
    "media_outlets = df['media_outlet'].unique()\n",
    "\n",
    "#Split into dictionaries\n",
    "\n",
    "Q1_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q1_outlet_df = Q1_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q1_outlet_dfs[outlet] = count_responses(Q1_outlet_df)\n",
    "\n",
    "Q2_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q2_outlet_df = Q2_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q2_outlet_dfs[outlet] = count_responses(Q2_outlet_df)\n",
    "\n",
    "Q3_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q3_outlet_df = Q3_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q3_outlet_dfs[outlet] = count_responses(Q3_outlet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert each DataFrame to a JSON-compatible format\n",
    "Q1_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q1_outlet_dfs.items()}\n",
    "Q2_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q2_outlet_dfs.items()}\n",
    "Q3_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q3_outlet_dfs.items()}\n",
    "\n",
    "\n",
    "# Save the dictionary of JSON data to a file\n",
    "with open(\"output/Q1_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q1_outlet_json, f, indent=4)\n",
    "\n",
    "with open(\"output/Q2_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q2_outlet_json, f, indent=4)\n",
    "\n",
    "with open(\"output/Q3_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q3_outlet_json, f, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
