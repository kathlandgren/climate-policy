{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pyprojroot import here\n",
    "\n",
    "import krippendorff\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import jaccard_score\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intercoder reliability for the second round of annotation \n",
    "\n",
    "This notebook computes the intercoder reliability for the second round of annotation using the bootstrapping method.\n",
    "Additionally, this notebook processes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the annotation JSON file into a DataFrame\n",
    "complete_df = pd.read_json(here(\"data/individual_policy_data/policy-recoding-2025-05-21-complete.json\"))\n",
    "df_with_inner_id = pd.read_json(here(\"data/individual_policy_data/policy-at-2025-04-27.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract internal_id from the nested 'data' field\n",
    "df_with_inner_id['internal_id'] = df_with_inner_id['data'].apply(lambda x: x['internal_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mapping from internal_id -> inner_id\n",
    "internal_to_inner = dict(zip(df_with_inner_id['internal_id'], df_with_inner_id['inner_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique transcripts with two or more annotators: 750\n"
     ]
    }
   ],
   "source": [
    "# Count the number of annotators per transcript\n",
    "annotator_counts = complete_df.groupby(\"internal_id\")[\"annotator\"].nunique()\n",
    "\n",
    "# Count transcripts where 2 or more annotators annotated the same transcript\n",
    "num_transcripts_with_multiple_annotators = (annotator_counts >= 2).sum()\n",
    "\n",
    "print(f\"Number of unique transcripts with two or more annotators: {num_transcripts_with_multiple_annotators}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute intercoder reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize policy column to always be a list\n",
    "def standardize_policy(entry):\n",
    "    if isinstance(entry, dict) and 'choices' in entry:\n",
    "        return entry['choices']\n",
    "    elif isinstance(entry, str):\n",
    "        return [entry]\n",
    "    elif isinstance(entry, list):\n",
    "        return entry\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply normalization\n",
    "complete_df['policy'] = complete_df['policy'].apply(standardize_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'updated_at' is in datetime format\n",
    "complete_df['updated_at'] = pd.to_datetime(complete_df['updated_at'])\n",
    "\n",
    "# Sort by internal_id, annotator, and updated_at (newest last)\n",
    "df_sorted = complete_df.sort_values(by=['internal_id', 'annotator', 'updated_at'])\n",
    "\n",
    "# Keep only the most recent annotation for each (internal_id, annotator) pair\n",
    "df_deduped = df_sorted.drop_duplicates(subset=['internal_id', 'annotator'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = complete_df[['internal_id','annotator','policy']]\n",
    "df = df_deduped[['internal_id','annotator','policy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define set of policies\n",
    "all_policies = [ \"Paris Agreement\", \"Green New Deal\", \"Executive action\", \"Emergency declaration\", \"(De)regulation and laws\", \"Renewable energy\", \"Emission reduction\", \"Oil and gas industry\", \"Other climate policy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode each policy annotation\n",
    "def one_hot_encode(policies, all_policies):\n",
    "    return [1 if policy in policies else 0 for policy in all_policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/mrdbnbz979z112bjxxhzr3zh0000gn/T/ipykernel_31313/457520858.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"policy_encoded\"] = df[\"policy\"].apply(lambda x: one_hot_encode(x, all_policies))\n"
     ]
    }
   ],
   "source": [
    "# Ensure `policy_encoded` is a DataFrame-friendly list\n",
    "df[\"policy_encoded\"] = df[\"policy\"].apply(lambda x: one_hot_encode(x, all_policies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list column into separate one-hot encoded columns\n",
    "policy_df = pd.DataFrame(df[\"policy_encoded\"].tolist(), columns=all_policies)\n",
    "df_binary = pd.concat([df.drop(columns=[\"policy_encoded\"]), policy_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of one-hot encoded policy columns\n",
    "policy_columns = all_policies  # Ensure this contains your policy labels\n",
    "\n",
    "# Pivot DataFrame (internal_id × annotator)\n",
    "df_pivot = df_binary.pivot(index=\"internal_id\", columns=\"annotator\", values=policy_columns)\n",
    "\n",
    "# Drop rows where all annotator responses are missing\n",
    "df_pivot = df_pivot.dropna(how=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kath/Documents/climate_policy/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 due to no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To store results\n",
    "jaccard_results = []\n",
    "\n",
    "# Group by transcript\n",
    "for transcript_id, group in df_binary.groupby('internal_id'):\n",
    "    # Each annotator's policy vector\n",
    "    annotator_vectors = group.set_index('annotator')[policy_columns]\n",
    "    \n",
    "    # Skip if only one annotator\n",
    "    if len(annotator_vectors) < 2:\n",
    "        continue\n",
    "\n",
    "    # Compute pairwise Jaccard for all annotator pairs\n",
    "    pair_scores = []\n",
    "    for a1, a2 in combinations(annotator_vectors.index, 2):\n",
    "        v1 = annotator_vectors.loc[a1].values\n",
    "        v2 = annotator_vectors.loc[a2].values\n",
    "        score = jaccard_score(v1, v2)\n",
    "        pair_scores.append(score)\n",
    "    \n",
    "    # Average pairwise Jaccard for the transcript\n",
    "    jaccard_results.append({\n",
    "        'internal_id': transcript_id,\n",
    "        'avg_jaccard': sum(pair_scores) / len(pair_scores),\n",
    "        'n_annotators': len(annotator_vectors)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "jaccard_df = pd.DataFrame(jaccard_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix zeros\n",
    "jaccard_df.loc[jaccard_df['avg_jaccard'] == 0, 'avg_jaccard'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9406730158730159)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_df['avg_jaccard'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By-policy Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_alpha(data, level_of_measurement='nominal', n_boot=10000, conf_level=0.95):\n",
    "    \"\"\"\n",
    "    Bootstraps Krippendorff’s alpha with confidence intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 2D numpy array (coders × items)\n",
    "    - level_of_measurement: 'nominal', 'ordinal', etc.\n",
    "    - n_boot: number of bootstrap samples\n",
    "    - conf_level: confidence level for CI (e.g., 0.95)\n",
    "\n",
    "    Returns:\n",
    "    - alpha_hat: point estimate of alpha\n",
    "    - ci_lower, ci_upper: confidence interval bounds\n",
    "    \"\"\"\n",
    "    n_items = data.shape[1]\n",
    "    alphas = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        sample_indices = np.random.choice(n_items, n_items, replace=True)\n",
    "        sample = data[:, sample_indices]\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(reliability_data=sample, level_of_measurement=level_of_measurement)\n",
    "        except Exception:\n",
    "            alpha = np.nan\n",
    "        alphas.append(alpha)\n",
    "\n",
    "    alphas = np.array(alphas)\n",
    "    alphas = alphas[~np.isnan(alphas)]\n",
    "    \n",
    "    alpha_hat = krippendorff.alpha(reliability_data=data, level_of_measurement=level_of_measurement)\n",
    "    ci_lower = np.percentile(alphas, (1 - conf_level) / 2 * 100)\n",
    "    ci_upper = np.percentile(alphas, (1 + conf_level) / 2 * 100)\n",
    "\n",
    "    return alpha_hat, ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paris Agreement\n",
      "  Krippendorff’s Alpha: 0.958\n",
      "  95% CI: (0.936, 0.978)\n",
      "  Number of Disagreements: 15\n",
      "\n",
      "Green New Deal\n",
      "  Krippendorff’s Alpha: 0.983\n",
      "  95% CI: (0.966, 0.996)\n",
      "  Number of Disagreements: 4\n",
      "\n",
      "Executive action\n",
      "  Krippendorff’s Alpha: 0.967\n",
      "  95% CI: (0.945, 0.986)\n",
      "  Number of Disagreements: 9\n",
      "\n",
      "Emergency declaration\n",
      "  Krippendorff’s Alpha: 0.943\n",
      "  95% CI: (0.845, 1.000)\n",
      "  Number of Disagreements: 2\n",
      "\n",
      "(De)regulation and laws\n",
      "  Krippendorff’s Alpha: 0.987\n",
      "  95% CI: (0.970, 1.000)\n",
      "  Number of Disagreements: 3\n",
      "\n",
      "Renewable energy\n",
      "  Krippendorff’s Alpha: 0.909\n",
      "  95% CI: (0.876, 0.938)\n",
      "  Number of Disagreements: 32\n",
      "\n",
      "Emission reduction\n",
      "  Krippendorff’s Alpha: 0.918\n",
      "  95% CI: (0.888, 0.946)\n",
      "  Number of Disagreements: 29\n",
      "\n",
      "Oil and gas industry\n",
      "  Krippendorff’s Alpha: 0.918\n",
      "  95% CI: (0.888, 0.945)\n",
      "  Number of Disagreements: 30\n",
      "\n",
      "Other climate policy\n",
      "  Krippendorff’s Alpha: 0.810\n",
      "  95% CI: (0.749, 0.866)\n",
      "  Number of Disagreements: 38\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "policy_results = {}\n",
    "\n",
    "# Iterate through each policy\n",
    "for policy in policy_columns:\n",
    "    policy_data = np.array(df_pivot[policy].T)  # Transpose to (coders × items)\n",
    "\n",
    "    # Bootstrap Krippendorff’s alpha and CI\n",
    "    alpha, ci_lower, ci_upper = bootstrap_alpha(policy_data, level_of_measurement='nominal')\n",
    "\n",
    "    # Identify disagreements\n",
    "    disagreements = []\n",
    "    for i, transcript in enumerate(df_pivot.index):\n",
    "        annotator_responses = policy_data[:, i]\n",
    "        annotator_responses = annotator_responses[~np.isnan(annotator_responses)]\n",
    "        if len(set(annotator_responses)) > 1:\n",
    "            disagreements.append(transcript)\n",
    "\n",
    "    # Store results\n",
    "    policy_results[policy] = {\n",
    "        \"krippendorff_alpha\": alpha,\n",
    "        \"alpha_ci_lower\": ci_lower,\n",
    "        \"alpha_ci_upper\": ci_upper,\n",
    "        \"num_disagreements\": len(disagreements),\n",
    "        \"disagreeing_transcripts\": disagreements\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for policy, results in policy_results.items():\n",
    "    print(f\"\\n{policy}\")\n",
    "    print(f\"  Krippendorff’s Alpha: {results['krippendorff_alpha']:.3f}\")\n",
    "    print(f\"  95% CI: ({results['alpha_ci_lower']:.3f}, {results['alpha_ci_upper']:.3f})\")\n",
    "    print(f\"  Number of Disagreements: {results['num_disagreements']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall policy Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff’s alpha (overall): 0.945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Melt the binary frame into long format\n",
    "policy_columns = ['Paris Agreement', 'Green New Deal', 'Executive action', 'Emergency declaration',\n",
    "                  '(De)regulation and laws', 'Renewable energy', 'Emission reduction',\n",
    "                  'Oil and gas industry', 'Other climate policy']\n",
    "\n",
    "df_long = df_binary.melt(\n",
    "    id_vars=['internal_id', 'annotator'],\n",
    "    value_vars=policy_columns,\n",
    "    var_name='policy',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Step 2: Define each item as a transcript-policy pair\n",
    "df_long['item'] = df_long['internal_id'].astype(str) + \"::\" + df_long['policy']\n",
    "\n",
    "# Step 3: Pivot to get item × annotator matrix\n",
    "df_matrix = df_long.pivot(index='item', columns='annotator', values='value')\n",
    "\n",
    "# Step 4: Check number of valid annotations per item\n",
    "annotation_counts = df_matrix.notna().sum(axis=1)\n",
    "\n",
    "# Step 5: Filter out items with fewer than 2 annotations\n",
    "df_filtered = df_matrix[annotation_counts >= 2]\n",
    "\n",
    "df_filtered = df_filtered.dropna(axis=1, how='all')\n",
    "\n",
    "# Step 6: Convert to numpy and compute alpha\n",
    "overall_alpha = krippendorff.alpha(\n",
    "    reliability_data=df_filtered.to_numpy().T,\n",
    "    level_of_measurement='nominal'\n",
    ")\n",
    "\n",
    "print(f\"Krippendorff’s alpha (overall): {overall_alpha:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kripp_df from existing policy_results\n",
    "kripp_df = pd.DataFrame.from_dict(policy_results, orient='index').reset_index()\n",
    "kripp_df = kripp_df.rename(columns={'index': 'Policy'})\n",
    "\n",
    "# Compute number of unique transcripts per policy using df_binary\n",
    "num_transcripts_per_policy = {\n",
    "    policy: df_binary.loc[df_binary[policy] == 1, 'internal_id'].nunique()\n",
    "    for policy in policy_columns\n",
    "}\n",
    "\n",
    "# Add accurate Num_transcripts based on df_binary counts\n",
    "kripp_df['num_transcripts'] = kripp_df['Policy'].map(num_transcripts_per_policy)\n",
    "\n",
    "# Drop the list of disagreements\n",
    "kripp_df = kripp_df.drop(columns=['disagreeing_transcripts'], errors='ignore')\n",
    "\n",
    "# Compute number of disagreements across all items\n",
    "overall_disagreements = df_filtered.apply(lambda row: len(set(row.dropna())) > 1, axis=1).sum()\n",
    "\n",
    "# Convert to numpy and compute alpha with CI\n",
    "matrix = df_filtered.to_numpy().T  # coders × items\n",
    "alpha, ci_lower, ci_upper = bootstrap_alpha(matrix, level_of_measurement='nominal', n_boot=10000)\n",
    "\n",
    "\n",
    "# Add overall alpha row\n",
    "kripp_df = pd.concat([\n",
    "    kripp_df,\n",
    "    pd.DataFrame([{\n",
    "        'Policy': 'TOTAL',\n",
    "        'krippendorff_alpha': overall_alpha,\n",
    "        \"alpha_ci_lower\": ci_lower,\n",
    "        \"alpha_ci_upper\": ci_upper,\n",
    "        'num_disagreements': overall_disagreements,\n",
    "        'num_transcripts': complete_df['internal_id'].nunique()\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "kripp_df.to_csv(\"output/individual_policy_krippendorff_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff’s alpha (overall): 0.945\n",
      "95% CI: (0.937, 0.953)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Krippendorff’s alpha (overall): {alpha:.3f}\")\n",
    "print(f\"95% CI: ({ci_lower:.3f}, {ci_upper:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique transcripts with disagreements (Krippendorff < 0.8): 0\n"
     ]
    }
   ],
   "source": [
    "# Find transcripts with disagreements for policies with alpha < 0.8\n",
    "disagreement_transcripts = set()\n",
    "\n",
    "for policy, results in policy_results.items():\n",
    "    if results[\"krippendorff_alpha\"] < 0.8:\n",
    "        disagreement_transcripts.update(results[\"disagreeing_transcripts\"])\n",
    "\n",
    "# Get the count of unique transcripts\n",
    "num_unique_disagreement_transcripts = len(disagreement_transcripts)\n",
    "\n",
    "print(f\"Number of unique transcripts with disagreements (Krippendorff < 0.8): {num_unique_disagreement_transcripts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of multiple mentions and \"other climate policy\" binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"yes\"/\"no\" to True/False\n",
    "complete_df['multiple-answer'] = complete_df['multiple-answer'].str.lower() == 'yes'\n",
    "complete_df['outside-answer'] = complete_df['outside-answer'].str.lower() == 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique internal_id count where multiple views are expressed is Yes: 79\n",
      "Unique internal_id count where policy is mentioned outside the relevant chunk is Yes: 46\n"
     ]
    }
   ],
   "source": [
    "# Count unique internal_id where \"multiple-answer\" is True\n",
    "unique_multiple_answer = complete_df.loc[complete_df['multiple-answer'], 'internal_id'].nunique()\n",
    "\n",
    "# Count unique internal_id where \"outside-answer\" is True\n",
    "unique_outside_answer = complete_df.loc[complete_df['outside-answer'], 'internal_id'].nunique()\n",
    "\n",
    "print(f\"Unique internal_id count where multiple views are expressed is Yes: {unique_multiple_answer}\")\n",
    "print(f\"Unique internal_id count where policy is mentioned outside the relevant chunk is Yes: {unique_outside_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_id = internal_to_inner.get(transcript_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'multiple-answer' is True\n",
    "multiple_explained_df = complete_df.loc[complete_df['multiple-answer'], ['internal_id', 'q3_coder_response', 'multiple_explained']]\n",
    "multiple_explained_df['inner_id'] = multiple_explained_df['internal_id'].map(internal_to_inner)\n",
    "\n",
    "cols = ['inner_id', 'internal_id', 'q3_coder_response', 'multiple_explained']\n",
    "multiple_explained_df = multiple_explained_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the filtered DataFrame\n",
    "multiple_explained_df.to_csv('output/multiple_policies_explained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'multiple-answer' is True\n",
    "outside_explained_df = complete_df.loc[complete_df['outside-answer'], ['internal_id', 'q3_coder_response', 'outside-policy', 'more-comments-explained']]\n",
    "outside_explained_df['inner_id'] = outside_explained_df['internal_id'].map(internal_to_inner)\n",
    "\n",
    "cols = ['inner_id','internal_id', 'q3_coder_response', 'outside-policy','more-comments-explained']\n",
    "outside_explained_df = outside_explained_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inner_id</th>\n",
       "      <th>internal_id</th>\n",
       "      <th>q3_coder_response</th>\n",
       "      <th>outside-policy</th>\n",
       "      <th>more-comments-explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Supports</td>\n",
       "      <td>{'choices': ['Paris Agreement', 'Green New Dea...</td>\n",
       "      <td>didn't add UN climate accord as other climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>nan</td>\n",
       "      <td>Oil and gas industry</td>\n",
       "      <td>May have to throw this one out because it does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13</td>\n",
       "      <td>77</td>\n",
       "      <td>Opposes</td>\n",
       "      <td>{'choices': ['Renewable energy', 'Oil and gas ...</td>\n",
       "      <td>Agree with Kate except for \"emergency declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13</td>\n",
       "      <td>77</td>\n",
       "      <td>Opposes</td>\n",
       "      <td>{'choices': ['Renewable energy', 'Oil and gas ...</td>\n",
       "      <td>Disagree with Zade because not referring to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>22</td>\n",
       "      <td>187</td>\n",
       "      <td>Opposes</td>\n",
       "      <td>(De)regulation and laws</td>\n",
       "      <td>Changed my coding to reflect election timeline...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    inner_id  internal_id q3_coder_response  \\\n",
       "1          1            0          Supports   \n",
       "17         9           36               nan   \n",
       "24        13           77           Opposes   \n",
       "25        13           77           Opposes   \n",
       "42        22          187           Opposes   \n",
       "\n",
       "                                       outside-policy  \\\n",
       "1   {'choices': ['Paris Agreement', 'Green New Dea...   \n",
       "17                               Oil and gas industry   \n",
       "24  {'choices': ['Renewable energy', 'Oil and gas ...   \n",
       "25  {'choices': ['Renewable energy', 'Oil and gas ...   \n",
       "42                            (De)regulation and laws   \n",
       "\n",
       "                              more-comments-explained  \n",
       "1   didn't add UN climate accord as other climate ...  \n",
       "17  May have to throw this one out because it does...  \n",
       "24  Agree with Kate except for \"emergency declarat...  \n",
       "25  Disagree with Zade because not referring to an...  \n",
       "42  Changed my coding to reflect election timeline...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outside_explained_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_explained_df.to_csv('output/outside_policy_explained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff’s Alpha for 'multiple-answer': 0.620\n",
      "Krippendorff’s Alpha for 'outside-answer': 0.543\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean values to integers (True -> 1, False -> 0)\n",
    "complete_df['multiple-answer'] = complete_df['multiple-answer'].astype(int)\n",
    "complete_df['outside-answer'] = complete_df['outside-answer'].astype(int)\n",
    "\n",
    "df_kripp = complete_df.pivot(index=\"internal_id\", columns=\"annotator\", values=['multiple-answer', 'outside-answer'])\n",
    "\n",
    "# Drop rows where all annotator responses are missing (NaN)\n",
    "df_kripp = df_kripp.dropna(how=\"all\")\n",
    "\n",
    "# Convert to numpy arrays and transpose for Krippendorff’s alpha (coders x items)\n",
    "alpha_multiple = krippendorff.alpha(reliability_data=np.array(df_kripp['multiple-answer']).T, level_of_measurement='nominal')\n",
    "alpha_outside = krippendorff.alpha(reliability_data=np.array(df_kripp['outside-answer']).T, level_of_measurement='nominal')\n",
    "\n",
    "# Print results\n",
    "print(f\"Krippendorff’s Alpha for 'multiple-answer': {alpha_multiple:.3f}\")\n",
    "print(f\"Krippendorff’s Alpha for 'outside-answer': {alpha_outside:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
