{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pyprojroot import here\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and format data for general policy analysis\n",
    "\n",
    "Includes data for acknowledgement of anthropogenic climate change, climate concern, and climate policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the annotation JSON file into a DataFrame\n",
    "df = pd.read_json(here(\"data/recoded_transcripts/main-at-2024-05-28-recoded.json\"))\n",
    "\n",
    "#get intercoder reliability IDs list\n",
    "all_inner_IDs=pd.read_csv(here(\"data/recoded_transcripts/main_project_IDs.csv\"))\n",
    "\n",
    "# Load JSON transcript data\n",
    "with open(here(\"data/transcripts/news_segments_2020-04_2021-04_unique_IDs.json\")) as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# Assuming each item in the list has a 'data' key, we extract those\n",
    "extracted_data = [item['data'] for item in data_list]\n",
    "\n",
    "# Normalize the data to flatten the nested structure\n",
    "transcript_df = pd.json_normalize(extracted_data)\n",
    "\n",
    "# unpack the meta_info\n",
    "expanded_columns = df['meta_info'].apply(pd.Series)\n",
    "df = pd.concat([df.drop('meta_info', axis=1), expanded_columns], axis=1)\n",
    "#filter the df based on index of annotations\n",
    "df = df[df['internal_id'].isin(all_inner_IDs[\"internal_id\"])]\n",
    "\n",
    "# define annotators\n",
    "annotator_list=df['annotator'].unique()\n",
    "annotator_dict = {\n",
    "    2: 'JE',\n",
    "    3: 'EK',\n",
    "    10: 'VR',\n",
    "    4: 'CA',\n",
    "    11: 'LU',\n",
    "    13: 'PI',\n",
    "    12: 'GE',\n",
    "    14: 'TR',\n",
    "    15: 'KA'\n",
    "}\n",
    "\n",
    "## create dataframe\n",
    "df= pd.merge(df, all_inner_IDs[['internal_id', 'project_id']], on='internal_id', how='left')\n",
    "df['media_outlet'] = df['source'].str.split().str[0]\n",
    "question_labels=['climate_change','attitude','policy']\n",
    "# Convert the 'meta_info.date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%B %d, %Y %A')\n",
    "\n",
    "\n",
    "df.to_csv(\"output/all_Q.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/mrdbnbz979z112bjxxhzr3zh0000gn/T/ipykernel_31705/1527111932.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Q1_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[0]).fillna(value=np.nan)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define separate dataframes for each question\n",
    "Q1_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[0]).fillna(value=np.nan)\n",
    "Q2_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[1])\n",
    "Q3_df = df.pivot(index='annotator', columns=['project_id','media_outlet'], values=question_labels[2])\n",
    "\n",
    "# Save DataFrames as CSV files in the output directory\n",
    "Q1_df.to_csv(\"output/Q1_df.csv\")\n",
    "Q2_df.to_csv(\"output/Q2_df.csv\")\n",
    "Q3_df.to_csv(\"output/Q3_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## define choices for each question\n",
    "Q1_choices=[\"Acknowledges\",\"Neutral\",\"Denies\",\"Debate\",\"Unclear\"]\n",
    "Q2_choices=[\"Expresses climate concern\",\"Neutral\",\"Expresses opposition to climate concern\",\"Debate\",\"Unclear\"]\n",
    "Q3_choices=[\"Supports\",\"Neutral\",\"Opposes\",\"Debate\",\"Unclear\",\"Does not mention\"]\n",
    "\n",
    "\n",
    "# define short choices for displaying\n",
    "short_Q2_choices=['Concern',\n",
    " 'Neutral',\n",
    " 'Opposition',\n",
    " 'Debate',\n",
    " 'Unclear'] \n",
    "short_Q3_choices=[\"Supports\",\"Neutral\",\"Opposes\",\"Debate\",\"Unclear\",\"No mention\"]\n",
    "\n",
    "\n",
    "# Save lists to the output directory\n",
    "with open(\"output/Q1_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q1_choices, f)\n",
    "\n",
    "with open(\"output/Q2_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q2_choices, f)\n",
    "\n",
    "with open(\"output/Q3_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q3_choices, f)\n",
    "\n",
    "with open(\"output/short_Q2_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(short_Q2_choices, f)\n",
    "\n",
    "with open(\"output/short_Q3_choices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(short_Q3_choices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count responses \n",
    "def count_responses(Q_df):\n",
    "   # Flatten the DataFrame to consider all the responses\n",
    "    responses = Q_df.values.flatten()\n",
    "    # Filter out NaN values\n",
    "    responses = responses[~pd.isnull(responses)]\n",
    "\n",
    "    # Count occurrences of each unique response\n",
    "    response_counts = pd.Series(responses).value_counts()\n",
    "\n",
    "    # Assign the value 0.5 to each response because there are two annotators per response\n",
    "    response_values = response_counts * 0.5\n",
    "\n",
    "    # Convert to DataFrame for better presentation\n",
    "    response_values_df = response_values.reset_index()\n",
    "    \n",
    "    response_values_df.columns = ['Response', 'Value']\n",
    "    # Calculating the total value\n",
    "    total_value = response_values_df['Value'].sum()\n",
    "\n",
    "    # Adding the Proportion column\n",
    "    response_values_df['Proportion'] = response_values_df['Value'] / total_value\n",
    "\n",
    "    return response_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_count_Q1=count_responses(Q1_df)\n",
    "response_count_Q2=count_responses(Q2_df)\n",
    "response_count_Q3=count_responses(Q3_df)\n",
    "\n",
    "response_count_Q1.to_csv(\"output/response_count_Q1.csv\",index=False)\n",
    "response_count_Q2.to_csv(\"output/response_count_Q2.csv\",index=False)\n",
    "response_count_Q3.to_csv(\"output/response_count_Q3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify unique media outlets\n",
    "media_outlets = df['media_outlet'].unique()\n",
    "\n",
    "#Split into dictionaries\n",
    "\n",
    "Q1_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q1_outlet_df = Q1_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q1_outlet_dfs[outlet] = count_responses(Q1_outlet_df)\n",
    "\n",
    "Q2_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q2_outlet_df = Q2_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q2_outlet_dfs[outlet] = count_responses(Q2_outlet_df)\n",
    "\n",
    "Q3_outlet_dfs={}\n",
    "for outlet in media_outlets:\n",
    "    # Selecting only the columns corresponding to the current media outlet\n",
    "    Q3_outlet_df = Q3_df.xs(outlet, level='media_outlet', axis=1)\n",
    "    Q3_outlet_dfs[outlet] = count_responses(Q3_outlet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert each DataFrame to a JSON-compatible format\n",
    "Q1_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q1_outlet_dfs.items()}\n",
    "Q2_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q2_outlet_dfs.items()}\n",
    "Q3_outlet_json = {outlet: df.to_dict(orient='records') for outlet, df in Q3_outlet_dfs.items()}\n",
    "\n",
    "\n",
    "# Save the dictionary of JSON data to a file\n",
    "with open(\"output/Q1_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q1_outlet_json, f, indent=4)\n",
    "\n",
    "with open(\"output/Q2_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q2_outlet_json, f, indent=4)\n",
    "\n",
    "with open(\"output/Q3_outlet_dfs.json\", \"w\") as f:\n",
    "    json.dump(Q3_outlet_json, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the response categories and their order\n",
    "\n",
    "Q1_choices_to_use=[\"Acknowledges\",\"Neutral\",\"Denies\",\"Debate\"]\n",
    "Q2_choices_to_use=[\"Expresses climate concern\",\"Neutral\",\"Expresses opposition to climate concern\",\"Debate\"]\n",
    "Q3_choices_to_use=[\"Supports\",\"Neutral\",\"Opposes\",\"Debate\",\"Does not mention\"]\n",
    "\n",
    "# Build the Q3_counts dictionary\n",
    "Q1_counts = {}\n",
    "Q2_counts = {}\n",
    "Q3_counts = {}\n",
    "\n",
    "\n",
    "# Function to extract counts\n",
    "def extract_counts(outlet_json, choices_to_use, Q3=False):\n",
    "    counts_dict = {}\n",
    "    for outlet, responses in outlet_json.items():\n",
    "        if Q3==False:\n",
    "            counts = [0, 0, 0, 0]\n",
    "        else:\n",
    "            counts = [0, 0, 0, 0, 0]\n",
    "        for resp in responses:\n",
    "            response_type = resp[\"Response\"]\n",
    "            value = resp[\"Value\"]\n",
    "            if response_type in choices_to_use:\n",
    "                idx = choices_to_use.index(response_type)\n",
    "                counts[idx] = value\n",
    "        counts_dict[outlet] = counts\n",
    "    return counts_dict\n",
    "\n",
    "# Build the counts dictionaries\n",
    "Q1_counts = extract_counts(Q1_outlet_json, Q1_choices_to_use)\n",
    "Q2_counts = extract_counts(Q2_outlet_json, Q2_choices_to_use)\n",
    "Q3_counts = extract_counts(Q3_outlet_json, Q3_choices_to_use, Q3=True)\n",
    "\n",
    "# Optionally dump to files\n",
    "with open(\"output/Q1_counts.json\", \"w\") as f:\n",
    "    json.dump(Q1_counts, f, indent=4)\n",
    "with open(\"output/Q2_counts.json\", \"w\") as f:\n",
    "    json.dump(Q2_counts, f, indent=4)\n",
    "with open(\"output/Q3_counts.json\", \"w\") as f:\n",
    "    json.dump(Q3_counts, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
